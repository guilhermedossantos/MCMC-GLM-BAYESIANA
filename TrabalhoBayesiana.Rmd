---
title: "Ajuste de Modelo Linear Generalizado Utilizando o Método de Monte Carlo via Cadeias de Markov"
author: "Guilherme dos Santos & Isabelle da Costa Oliveira"
date: "03 de julho de 2019"
language:
  label:
    fig: "Figura"
    tab: "Tabela"
output:
  pdf_document: default
  html_notebook: default

---

```{r include=FALSE}
knitr::opts_knit$set(root.dir = "C:\\Users\\ibge\\Downloads\\Bayesiana\\MCMC-GLM-BAYESIANA-master")

knitr::opts_chunk$set(echo = FALSE)

library(coda)
library(kableExtra)

load("convergiu_3mi.RData")
```

# Introdução
Estamos interessados no ajuste de um modelo linear generalizado usando distribuição Poisson, a função de ligação canônica e o predito linear $\eta_i = \beta_1 + \beta_2i + \beta_3i^2, i = 1, ..., 13$ para o número de casos de AIDS na Bélgica por ano, de 1981 a 1993. Para tal, iremos considerar distribuições a priori independentes e pouco informativas para \textbf{$\theta$} $= (\beta_1, \beta_2, \beta_3)$.

# Distribuições 

Sabemos que:\\
    $$p(\theta|\underline{y})\propto p(\underline{y}|\theta)p(\theta)$$
e 
    $$p(\underline{y}|\theta)= \Pi_{i=1}^{13}\frac{\exp\{y_i(\beta_1 + \beta_2i \}\exp\{-\sum_{i=1}^{13}{\exp(\beta_1 + \beta_2i}\}}{y_i!}$$.
    
Propondo $\beta_1, \beta_2$ independentes e identicamentes distribuídas a priri com $\beta_i \sim N(0,k), i = 1,2$. Com variância $k = 200$ para q tenhamos uma distribuição a priori vaga.

Portanto, \\

$$p(\theta|\underline{y}) \propto \Pi_{i=1}^{13} \exp\{y_i(\beta_1 + \beta_2i)\}\exp\{-\sum_{i=1}^{13}{\exp(\beta_1 + \beta_2i )}\}exp{-(\frac{\beta_1}{2k} + \frac{\beta_2}{2k})}$$ 
é o núcleo da distribuição posteriori desejada.

E (escrever condicionais completas)



    
#Algotitmo de Gibbs usando Metropolis-Hasting para gerar condicionais completas

## Dados
```{r}
#dados
n <- 13

x <- 1:13
y <- c(12, 14, 33, 50, 67, 74, 123, 141, 165, 204, 253, 246, 240)
```

## Estimativa clássica 

### Estimativa pontual
```{r}
modelo <- glm(y ~ x, family = poisson)
summary(modelo)
```

### Intervalos de confiança clássicos
```{r warning=FALSE, message=FALSE}
confint(modelo)
```

## MCMC

```{r eval = F}
#número de iterações
N <- 3*10^6

#variância da distribuição a priori proposta
k <- 200

#contadores 
cont_b1 <- 0
cont_b2 <- 0
cont_b3 <- 0

## Condicionais completas
cc_b1 <- function(b1, b2){sum(y*(b1 + b2*x)) - sum(exp(b1 + b2*x)) + ((-1/(2*k))*(b1^2))}
cc_b2 <- function(b1, b2){sum(y*(b1 + b2*x)) - sum(exp(b1 + b2*x)) + ((-1/(2*k))*(b2^2))}


```

```{r eval = F}
#valores iniciais
b1_am <- b2_am <- b3_am <- rep(NA, N)
b1_am[1] <- b2_am[1] <- 2

for(i in 2:N){
    
     #metropolis para b1
     b1 <- rnorm(1, b1_am[i-1], 2) 
     alpha <- min(0, 
                  cc_b1(b1, b2_am[i-1]) - cc_b1(b1_am[i-1], b2_am[i-1]))
     
     if(log(runif(1)) < alpha){
       b1_am[i] <- b1 
       cont_b1 <- cont_b1+1
     }else{
         b1_am[i] <- b1_am[i-1]
         }  
     
     #metropolis para b2
     b2 <- rnorm(1, b2_am[i-1], 1)
     alpha2 <- min(0, 
                   cc_b2(b1_am[i], b2) - cc_b2(b1_am[i], b2_am[i-1]))
     
     if(log(runif(1)) < alpha2){
       b2_am[i] <- b2
       cont_b2 <- cont_b2+1
     }else{
         b2_am[i] <- b2_am[i-1]
         }  
  
}
```

## Tratamento da cadeia e verificação de convergência

#### Gráfico da cadeia de $\beta_1$
```{r}
plot(b1_am, type = "l", 
     lwd = 2, main = expression(beta[1]), 
     ylab = expression(beta[1]),
     xlab = "Iteração")

```

#### Gráfico da cadeia de $\beta_2$
```{r}
plot(b2_am, type = "l", lwd = 2, 
     main = expression(beta[2]),
     ylab = expression(beta[2]),
     xlab = "Iteração")

```
  
  Inspecionando visualmente vemos que ambas as cadeias apresetam convergência. A taxa de aceitação para a cadeira de $\beta_1$ foi de aproximadamente `r round(cont_b1/N,3)`, e para a cadeia de $\beta_2$ foi `r round(cont_b2/N,3)`
  
  
  A seguir apresentamos alguns tratamentos realizados e o uso do critério de Geweke para verificação de convergência.
  

### Retirada do período de aquecimento e espaçamento da cadeia

```{r}
burnin <- 200000
b1_2 <- b1_am[(burnin + 1):N]
b2_2 <- b2_am[(burnin + 1):N]
#b3_2 <- b3_am[(burnin + 1):N]
```

  Consideramos as primeiras `r burnin` observações da cadeia como período de aquecimento. Abaixo seguem os Gráficos das cadeias após a retirada do período de aquecimento.

```{r}
par(mfrow = c(1,2))
plot(b1_2, type = "l", lwd = 2, main = expression(beta[1]), ylab = "", xlab = "")
plot(b2_2, type = "l", lwd = 2, main = expression(beta[2]), ylab = "", xlab = "")
```

O critério de Raftery (utilizando a função `raftery.diag`) retorna um alto fator de dependência fazendo-se necessário o espaçamento (\textit{thinning}). Com o objetivo de obter uma amostra de tamanho 5000 da posteriori, utilizamos um espaçamento de `r (N-burnin)/5000`.

```{r}
thin <- 560
b1_2<-b1_2[seq(1,length(b1_2),by=thin)]
b2_2<-b2_2[seq(1,length(b2_2),by=thin)]
```

Abaixo seguem os resultados dos critérios de Raftery-lewis e Geweke para verificação de convergência e autocorrelação na cadeia.

```{r}
raftery.diag(b1_2)
```

```{r}
raftery.diag(b2_2)
```

  Como visto acima, ambos o fator de dependência para ambas as cadeias após espaçamento é menor que 5, indicando um resultado aceitável.
  
  Abaixo vemos os resultados do critério de Geweke para ambos os parâmetros.
  
```{r}
geweke.diag(b1_2)
geweke.diag(b2_2)
```
  
  Em ambos os casos o resultado de interesse está entre -2 e 2, indicando convergência da cadeia.
  
## Estimativas pontuais e Intervalares
  Como a estimativa pontual do parâmetro depende da função de perda escolhida, aqui optamos por mostrar a média, moda e mediana a posteriori. Uma vez que estas são as estimativas quando as funções de perda quadrática, zero-um e absoluta são usadas, respectivamente, que são funções de perda frequentemente usadas.

### Pontuais
 - $\beta_1$
 
```{r}

moda_b1_2 <- as.numeric(names(which.max(table(b1_2))))

rbind.data.frame(mean(b1_2), 
                 median(b1_2),
                 moda_b1_2) -> est_pont_b1

colnames(est_pont_b1) <- "Estimativa"

rownames(est_pont_b1) <- c("Média", "Mediana", "Moda")

est_pont_b1 %>% 
  kable("latex") %>% 
  kable_styling(full_width = F)
```

 - $\beta_2$
```{r}
# mean(b2_2)
# median(b2_2)
moda_b2_2 <- as.numeric(names(which.max(table(b2_2))))

rbind.data.frame(mean(b2_2), 
                 median(b2_2),
                 moda_b2_2) -> est_pont_b2

colnames(est_pont_b2) <- "Estimativa"

rownames(est_pont_b2) <- c("Média", "Mediana", "Moda")

est_pont_b2 %>% 
  kable("latex") %>% 
  kable_styling(full_width = F)
```



### Intervalos de confiança HPD para $\beta_0$ e $\beta_1$

 - $\beta_1$
```{r}
library(TeachingDemos)

HPD_beta_1 <- emp.hpd(as.mcmc(b1_2))
names(HPD_beta_1) <- c("2.5%", "97.5%")
t(HPD_beta_1[1:2]) %>% kable("latex") %>% kable_styling(full_width = F)
```

 - $\beta_2$
```{r}
HPD_beta_2 <- emp.hpd(as.mcmc(b2_2))
names(HPD_beta_2) <- c("2.5%", "97.5%")
t(HPD_beta_2[1:2]) %>% kable("latex") %>% kable_styling(full_width = F)
```


## Distribuições a posteriori

 Nos gráficos abaixo vemos os histogramas das distribuições a posteriori de $\beta_1$ e $\beta_2$. A linha tracejada vertical é a estimativa clássica, os pontos no eixo $x$ correspondem aos intervalos de confiança e à média a posteriori (estimativa sob perda quadrática). A linha próxima ao eixo x representa a priori.
 
```{r}
par(mfrow=c(1,2))
hist(b1_2,prob=T,ylab="",xlab="",main=expression(beta[1]))
abline(v=coef(modelo)[1],col = 2,lty = 2,lwd = 3)
points(mean(b1_2), 0, col = 3, lwd = 4)
points(HPD_beta_1[1], 0, col = 3, lwd = 4)
points(HPD_beta_1[2], 0, col = 3, lwd = 4)
curve(dnorm(x, 0, sqrt(k)), col = 4, add = T)

hist(b2_2,prob=T,ylab="",xlab="",main=expression(beta[2]))
abline(v=coef(modelo)[2],col=2,lty=2,lwd=3)
points(mean(b2_2),0,col=3,lwd=4)
points(HPD_beta_2[1],0,col=3,lwd=4)
points(HPD_beta_2[2],0,col=3,lwd=4)
curve(dnorm(x, 0, sqrt(k)), col = 4, add = T)

```

## Conclusões

  Os algoritmos não apresentaram dificuldades para atingir a convergência. No entanto, as observações estavam altamente correlacionadas, daí, foi necessário realizar um grande número de iterações a fim de utilizar um espaçamento grande para diminuir a autocorrelação da cadeia.

  Vimos que o resultado Bayesiano nesse caso se aproxima do resultádo clássico, o que era esperado uma vez que utilizamos uma distribuição a priori vaga.


